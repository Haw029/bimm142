---
title: "Class 7 Machine Learning 1"
author: "Forrest Wang"
format: pdf
---

# K-means clustering

First we will test how this method works in R with some made up data.
```{r}
x <- rnorm(10000)
hist(x)
```

Let's make some numbers centered on -3
```{r}
tmp <- c(rnorm(30, -3), rnorm(30, +3))

x <- cbind(x=tmp, y=rev(tmp))
           
plot (x)
```

Now let's see how 'kmeans()' works with this data^
```{r}
km <- kmeans(x, centers = 2, nstart=20)
km
```

> How many points are in each clurster?
```{r}
km$size
```

> What 'component' of your result object details
  - cluster assignment/membership
  
```{r}
km$cluster
```
  
> What 'component' of your result object details
  - cluster center

```{r}
km$centers
```

> Plot x colored bt kmeans cluster assignment & add clusters as blue points

```{r}
plot(x, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=1.5)
```

#Hierarchical Clustering

The 'hclust()' function in R performs hierarchical clustering.

The 'hclus()' function requires an input distance matrix, which I can get from the 'dist()' function
```{r}
hc <- hclust( dist (x) )
hc
```

There is a plot() method for hclust objects:


```{r}
plot(hc)
```

Now to get my cluster membership vector, I need to "cut" the tree to yield sreperate "branches" with the "leaves" on each branch being out clusters. TO do this we use the 'cutree()' function.
```{r}
cutree(hc, h=8)
```

Use 'cutree()' with a k=2 (more useful)
```{r}
grps <- cutree(hc, k=2)
```

A plot of our data colored by our hclust 
```{r}
plot(x, col=grps)
```

#Principal Component Analysis (PCA)
Data:
```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)

head(x)
```

Q1:How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?
```{r}
dim(x)
nrow(x)
```
A1: There are 17 rows and 5 columns



Q2: Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?
A2: Insert inputs to edit rows inside the initial URL csv command to manipulate data directly as its pulled into R. 




Spotting Diff/Trends: 

Bar Plot
```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

```{r}
barplot(as.matrix(x), beside=FALSE, col=rainbow(nrow(x)))
```
Q3: Changing what optional argument in the above barplot() function results in the following plot?
A3: Changing beside from "beside=T" to "beside=FALSE" will change a bar plot to a cumulative plot that stacks all the data of each country which isn't that helpful. 

Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?
A5:The pairwise plot allows 1 on 1 comparisons of each country. If a given point lies on a diagonal for a given plot, a diagonal spread of points represents an average trend ("as x increases, so does y"), this means that said point is expected




Pairs Plot
```{r}
pairs(x, col=rainbow(10), pch=16)
```
Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?
A6: Northern Ireland points are much more right, especially the blue point, "maybe more alcohol" says Prof.B. 

While this is kind of useful it takes work to dig into the details here to find out what is the difference in these countries

## PCA to the rescue

Principal Component Analysis (PCA for short) can be a big help in these cases where we have lots of things that are being measured in a dataset (lots of dimensions).

The main PCA function in base R is called 'prcomp()'

The 'prcomp()' function wants as input the transpose of our food matrix/table/data.frame.

```{r}
pca <- prcomp( t(x) )
summary(pca)
```

The above result shows PCA captures 67% of the total variance in the original data in one PC and 96.5% in two PCs.

```{r}
attributes(x)
```


```{r}
head(pca$x)
```



Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

Let's plot our main results
```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", col=c("orange", "red", "blue", "darkgreen"))
text(pca$x[,1], pca$x[,2], colnames(x), col=c("orange", "red", "blue", "darkgreen"))
```
Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.
A8: Done^



DIGGING DEEPER (Variable Loading)

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

Q9:Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?
A9: The two prominent food groups highlighted by the plot above are soft drinks and fresh potatoes



#2. PCA of RNA-seq
```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```
Q10: How many genes and samples are in this data set?
Q10: There are 6 genes and 10 samples in the data set.




























